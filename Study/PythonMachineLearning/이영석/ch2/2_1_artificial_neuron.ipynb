{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 인공뉴런 : 초기 머신러닝의 간단한 역사\n",
    "\n",
    "- **맥컬록-피츠(MCP) 뉴런**\n",
    "    - AI를 설계하기 위해 생물학적 뇌가 동작하는 방식을 이해하려는 시도로, 1943년 워런 맥컬록, 월터피츠는 처음으로 간소화된 뇌의 뉴런개념을 발표하였는데 이를 맥컬록-피츠(MCP) 뉴런이라고 한다.\n",
    "    - 맥컬록과 피츠는 신경 세포를 이진 출력을 내는 간단한 논리회로로 표현하였다.\n",
    "\n",
    "- **퍼셉트론**\n",
    "    - 프랑크 로젠블라트는 MCP 뉴런 모델을 기반으로 퍼셉트론 학습 개념을 처음 발표하였다.\n",
    "    - 퍼셉트론 규칙에서 로젠블라트는 자동으로 최적의 가중치를 학습하는 알고리즘을 제안하였다.\n",
    "    - 이 가중치는 뉴런의 출력 신호를 낼지 말지를 결정하기 위해 입력 특성에 곱하는 계수이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 인공 뉴런의 수학적 정의\n",
    "\n",
    "- **인공 뉴런**\n",
    "    - 인공 뉴런(aritificial neuron) 아이디어는 2개의 클래스(+1과 -1)가 있는 이진 분류(binary classification)작업으로 볼 수 있다.\n",
    "    - 입력 값 $\\pmb x$, 가중치 벡터 $\\pmb w$, 최종 입력 $\\pmb z$, 결정함수 $\\pmb \\phi(\\pmb z)$로 나타낼 수 있음\n",
    "        - $\\pmb x = \\begin{bmatrix} x_1\\\\ \\dots \\\\ x_m\\end{bmatrix}, \\pmb w = \\begin{bmatrix}\\mathit w_1\\\\ \\dots\\\\ \\mathit w_m\\end{bmatrix} 일때, \\pmb z = \\pmb w^T \\pmb x $\n",
    "        - $\\pmb \\phi(\\pmb z) = \\begin{cases}1  & z \\ge \\theta \\\\-1 & \\text{그 외}\\end{cases}$ \n",
    "    - 퍼셉트론 알고리즘에서는 특정 샘플 $\\pmb x^{(i)}$의 최종 입력이 임계 값 $\\theta$보다 크면 1, 그렇지 않으면 -1로 예측한다.\n",
    "    - 따라서, 결정함수 $\\pmb \\phi(\\pmb z)$는 **단위 계단 함수(unit step function)**를 변형한 것이다.\n",
    "    - 위 식을 간단히 하기 위해 임계 값 $\\theta$를 식의 왼쪽으로 옮겨, $\\mathit w_0 = -\\theta$이고, $x_0 = 1$인 0번째 가중치를 정의하면 $\\mathit z$는 다음과 같다.\n",
    "        - $\\mathit z = \\mathit w_0 x_0 + \\mathit w_1 x_1 + \\dots + \\mathit w_m x_m$ (∵ $\\mathit w_0 x_0 = -\\theta)$\n",
    "        - $\\pmb \\phi(\\pmb z) = \\begin{cases}1  & z \\ge 0 \\\\-1 & \\text{그 외}\\end{cases}$\n",
    "    - 머신러닝 분야에서는 음수 임계 값 또는 가중치 $\\mathit w_0 = -\\theta$를 **절편**이라고 한다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 퍼셉트론 학습 규칙\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 로젠블라트의 초기 퍼셉트론 학습 규칙\n",
    "    - 가중치를 0 또는 랜덤한 작은 값으로 초기화\n",
    "    - 각 훈련 샘플 $\\pmb x^{(i)}$에서 다음 작업을 수행\n",
    "        - 출력 값(예측) $\\hat y$를 계산\n",
    "        - 가중치를 업데이트\n",
    "  \n",
    "  \n",
    "- 퍼셉트론 알고리즘\n",
    "    - $w^{(i)} = w^{(i-1)} + \\Delta w^{(i)}$\n",
    "    - $\\Delta w^{(i)} = \\eta (y^{(i)} - \\hat y^{(i)}) x^{(i)}$\n",
    "    - $\\therefore w^{(i)} = w^{(i-1)} +  \\eta (y^{(i)} - \\hat y^{(i)}) x^{(i)}$\n",
    "        - $\\eta$ : 학습률(learning rate)\n",
    "        - $y^{(i)}$ : $i$번째 훈련 샘플의 진짜 클래스 레이블(true class label)\n",
    "        - $\\hat y^{(i)}$ : $i$번째 훈련 샘플의 예측 클래스 레이블(predicted class label)\n",
    "\n",
    "\n",
    "- 가중치 벡터의 모든 가중치는 동시에 업데이트 됨\n",
    "    - 위의 2번째 식($\\Delta w^{(i)} = \\eta (y^{(i)} - \\hat y^{(i)}) x^{(i)}$)에서 $x^{(i)}$벡터라서 스칼라인 $\\eta (y^{(i)} - \\hat y^{(i)})$값과 곱해져 결과가 벡터임\n",
    "    - 이 벡터가 업데이트 되기 때문에, 가중치 벡터 $\\pmb w$에 있는 개별 가중치 $w_j$가 한번에 업데이트 된다고 표현한 것임\n",
    "\n",
    "\n",
    "- 퍼셉트론 작동 규칙\n",
    "    - 정확히 예측한 경우\n",
    "        - 정답 -1, 예측 -1 : $\\Delta w_j = \\eta (-1-(-1)) x_j^{(i)} = 0$\n",
    "        - 정답 1, 예측 1 : $\\Delta w_j = \\eta (1-(1)) x_j^{(i)} = 0$\n",
    "        \n",
    "    - 잘못 예측한 경우\n",
    "        - 가중치 업데이트 방향\n",
    "            - 정답 1, 예측 -1 : $\\Delta w_j = \\eta (1-(-1)) x_j^{(i)} = \\eta(2) x_j^{(i)}$ ($\\therefore$ 답이 1인데 -1로 예측시 +방향으로 업데이트)\n",
    "            - 정답 -1, 예측 1 : $\\Delta w_j = \\eta (-1-(1)) x_j^{(i)} = \\eta(-2) x_j^{(i)}$ ($\\therefore$ 답이 -1인데 1로 예측시 -방향으로 업데이트)\n",
    "        - 가중치 업데이트 크기\n",
    "            - $x_j^{(i)}=2$, 정답 1, 예측 -1 : $\\Delta w_j = (1-(-1))2 = (2)2 = 4$ ($\\therefore$ 가중치 업데이트 크기는 $x_j^{(i)}$값에 비례)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/perceptron_algorithm.png\" alt=\"perceptron_algorithm\" style=\"zoom:50%\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 퍼셉트론은 두 클래스가 선형적으로 구분되고 학습률이 충분히 작을 때에만 수렴이 보장됨\n",
    "- 두 클래스를 선형 결정 경계로 나눌 수 없다면 훈련을 반복할 최대 횟수(epoch)를 지정하고 분류 허용 오차를 지정할 수 있음\n",
    "- 그렇지 않으면 퍼셉트론은 가중치 업데이트를 멈추지 않음"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
